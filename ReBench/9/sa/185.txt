#[version = "0.0.5"]
def @main(%x0: Tensor[(1, 3, 224, 224), int8] /* ty=Tensor[(1, 3, 224, 224), int8] span=from_string:3:23 */, %x1: Tensor[(1, 3, 224, 224), int8] /* ty=Tensor[(1, 3, 224, 224), int8] span=from_string:14:24 */, %x2: Tensor[(3, 3, 3, 3), float32] /* ty=Tensor[(3, 3, 3, 3), float32] span=from_string:9:22 */, %x3: Tensor[(3, 3, 1, 1), float32] /* ty=Tensor[(3, 3, 1, 1), float32] span=from_string:12:22 */, %x4: Tensor[(3, 3, 3, 3), float32] /* ty=Tensor[(3, 3, 3, 3), float32] span=from_string:55:57 */, %x5: Tensor[(1, 3, 224, 89), int8] /* ty=Tensor[(1, 3, 224, 89), int8] span=from_string:18:24 */, %x6: Tensor[(1, 3, 224, 32), int8] /* ty=Tensor[(1, 3, 224, 32), int8] span=from_string:19:24 */, %x7: Tensor[(1, 3, 224, 4), int8] /* ty=Tensor[(1, 3, 224, 4), int8] span=from_string:20:24 */, %x8: Tensor[(224), float32] /* ty=Tensor[(224), float32] span=from_string:37:13 */, %x9: Tensor[(224), float32] /* ty=Tensor[(224), float32] span=from_string:42:18 */, %x10: Tensor[(224), float32] /* ty=Tensor[(224), float32] span=from_string:44:18 */, %x11: Tensor[(224), float32] /* ty=Tensor[(224), float32] span=from_string:40:24 */) -> (Tensor[(1, 3, 224, 224), float16], Tensor[(1, 3, 224, 224), float32], Tensor[(1, 3, 224, 349), int8], Tensor[(1, 3, 224, 224), float32], Tensor[(3, 3, 3, 3), float32], Tensor[(1, 224, 3, 224), float32], Tensor[(1, 1, 3, 224, 224), int8]) {
  %0 = qnn.dequantize(%x0, 0.1f /* ty=float32 span=from_string:3:32 */, 0 /* ty=int32 span=from_string:3:74 */);
  %1 = nn.softmax(%0);
  %2 = zeros(shape=[1], dtype="float32");
  %3 = add(%1, %2);
  let %x14: Tensor[(1, 3, 224, 224), float32] /* ty=Tensor[(1, 3, 224, 224), float32] span=from_string:8:12 */ = add(2f /* ty=float32 span=from_string:7:120 */, %3);
  %4 = add(%x14, 1f /* ty=float32 span=from_string:8:20 */);
  %5 = nn.conv2d(%4, %x2, padding=[1, 1, 1, 1], channels=3, kernel_size=[3, 3]);
  %6 = add(1f /* ty=float32 span=from_string:10:14 */, %5);
  %7 = add(%5, %6);
  %8 = nn.conv2d(%7, %x3, padding=[0, 0, 0, 0], channels=3, kernel_size=[1, 1]);
  %9 = nn.conv2d(%7, %x4, padding=[1, 1, 1, 1], channels=3, kernel_size=[3, 3]);
  %10 = qnn.dequantize(%x1, 0.2f /* ty=float32 span=from_string:14:33 */, 0 /* ty=int32 span=from_string:14:76 */);
  %11 = multiply(%0, %10);
  %12 = qnn.quantize(%11, 20f /* ty=float32 span=from_string:16:30 */, 0 /* ty=int32 span=from_string:16:73 */, out_dtype="int8");
  %13 = qnn.dequantize(%12, 0.5f /* ty=float32 span=from_string:17:33 */, 0 /* ty=int32 span=from_string:17:76 */);
  %14 = qnn.dequantize(%x5, 1.5f /* ty=float32 span=from_string:18:33 */, 0 /* ty=int32 span=from_string:18:76 */);
  %15 = qnn.dequantize(%x6, 2.5f /* ty=float32 span=from_string:19:33 */, 0 /* ty=int32 span=from_string:19:76 */);
  %16 = qnn.dequantize(%x7, 3.5f /* ty=float32 span=from_string:20:33 */, 0 /* ty=int32 span=from_string:20:76 */);
  %17 = (%13, %14, %15, %16) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(1, 3, 224, 89), float32], Tensor[(1, 3, 224, 32), float32], Tensor[(1, 3, 224, 4), float32]) span=from_string:22:21 */;
  %18 = concatenate(%17, axis=-1);
  %19 = layout_transform(%11, src_layout="NHWC", dst_layout="NCHW");
  %20 = layout_transform(%19, src_layout="NCHW", dst_layout="NCHW4c");
  %21 = nn.relu(%20);
  %22 = layout_transform(%21, src_layout="NCHW4c", dst_layout="NCHW");
  %23 = add(%x8, 1e-05f /* ty=float32 span=from_string:27:24 */);
  %24 = sqrt(%23);
  %25 = divide(1f /* ty=float32 span=from_string:29:18 */, %24);
  %26 = multiply(%25, %x11);
  %27 = expand_dims(%26, axis=1, num_newaxis=2);
  %28 = negative(%x9);
  %29 = multiply(%28, %26);
  %30 = add(%29, %x10);
  %31 = multiply(%22, %27);
  %32 = expand_dims(%30, axis=1, num_newaxis=2);
  %33 = add(%x8, 1e-05f /* ty=float32 span=from_string:37:24 */);
  %34 = sqrt(%33);
  %35 = divide(1f /* ty=float32 span=from_string:39:18 */, %34);
  %36 = multiply(%35, %x11);
  %37 = expand_dims(%36, axis=1, num_newaxis=2);
  %38 = negative(%x9);
  %39 = multiply(%38, %36);
  %40 = add(%39, %x10);
  %41 = multiply(%22, %37);
  %42 = expand_dims(%40, axis=1, num_newaxis=2);
  %43 = add(%31, %32);
  %44 = add(%41, %42);
  %45 = qnn.dequantize(%12, 2f /* ty=float32 span=from_string:49:31 */, 0 /* ty=int32 span=from_string:49:74 */);
  %46 = expand_dims(%45, axis=1);
  %47 = cast(%1, dtype="float16");
  %48 = add(%8, %9);
  %49 = qnn.quantize(%18, 3.5f /* ty=float32 span=from_string:53:31 */, 0 /* ty=int32 span=from_string:53:74 */, out_dtype="int8");
  %50 = layout_transform(%22, src_layout="NCHW", dst_layout="NHWC");
  %51 = add(2f /* ty=float32 span=from_string:55:15 */, %x4);
  %52 = add(%43, %44);
  %53 = qnn.quantize(%46, 2f /* ty=float32 span=from_string:57:29 */, 0 /* ty=int32 span=from_string:57:72 */, out_dtype="int8");
  (%47, %48, %49, %50, %51, %52, %53) /* ty=(Tensor[(1, 3, 224, 224), float16], Tensor[(1, 3, 224, 224), float32], Tensor[(1, 3, 224, 349), int8], Tensor[(1, 3, 224, 224), float32], Tensor[(3, 3, 3, 3), float32], Tensor[(1, 224, 3, 224), float32], Tensor[(1, 1, 3, 224, 224), int8]) span=from_string:58:3 */
}
